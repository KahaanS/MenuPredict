{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Meal', 'Item1', 'Item2', 'Item3', 'Item4', 'Item5', 'Item6', 'Item7', 'Item8'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DaySin</th>\n",
       "      <th>DayCos</th>\n",
       "      <th>MealNum</th>\n",
       "      <th>Item1Num</th>\n",
       "      <th>Item2Num</th>\n",
       "      <th>Item3Num</th>\n",
       "      <th>Item4Num</th>\n",
       "      <th>Item5Num</th>\n",
       "      <th>Item6Num</th>\n",
       "      <th>Item7Num</th>\n",
       "      <th>Item8Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08/08/22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08/08/22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/08/22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/08/22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09/08/22</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>15/07/23</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>3</td>\n",
       "      <td>184</td>\n",
       "      <td>245</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>16/07/23</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>201</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>16/07/23</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>88</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>16/07/23</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>2</td>\n",
       "      <td>234</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>16/07/23</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>3</td>\n",
       "      <td>238</td>\n",
       "      <td>186</td>\n",
       "      <td>278</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1344 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date    DaySin    DayCos  MealNum  Item1Num  Item2Num  Item3Num  \\\n",
       "0     08/08/22  0.000000  1.000000        0         0         0         0   \n",
       "1     08/08/22  0.000000  1.000000        1         1         1         1   \n",
       "2     08/08/22  0.000000  1.000000        2         2         2        -1   \n",
       "3     08/08/22  0.000000  1.000000        3         3         3         2   \n",
       "4     09/08/22  0.781831  0.623490        0         4         4         3   \n",
       "...        ...       ...       ...      ...       ...       ...       ...   \n",
       "1339  15/07/23 -0.974928 -0.222521        3       184       245         7   \n",
       "1340  16/07/23 -0.781831  0.623490        0         0        53       201   \n",
       "1341  16/07/23 -0.781831  0.623490        1        15        36        88   \n",
       "1342  16/07/23 -0.781831  0.623490        2       234        25        -1   \n",
       "1343  16/07/23 -0.781831  0.623490        3       238       186       278   \n",
       "\n",
       "      Item4Num  Item5Num  Item6Num  Item7Num  Item8Num  \n",
       "0            0         0         0         0         0  \n",
       "1            1         1         1         1        -1  \n",
       "2           -1        -1        -1        -1        -1  \n",
       "3            2         2         2         2         1  \n",
       "4            3         0         3         0         0  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "1339        26        16         1         2        -1  \n",
       "1340        79         0         3         0         0  \n",
       "1341        27        16         7         1        -1  \n",
       "1342        -1        -1        -1        -1        -1  \n",
       "1343        36        12        -1        -1        17  \n",
       "\n",
       "[1344 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/rllbqfts1ngcl7f8ts__f5000000gn/T/ipykernel_12327/275728622.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['Date'] = pd.to_datetime(data['Date'])\n"
     ]
    }
   ],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.drop(['Date'], axis=1, inplace=True)\n",
    "numarr = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(numarr[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for arr in numarr:\n",
    "#    arr[0] = arr[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1344, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numarr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1075, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = numarr[:int(numarr.shape[0]*0.8)]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(269, 11)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = numarr[int(numarr.shape[0]*0.8):]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "Y_train_list = []\n",
    "hops = 28\n",
    "no_records = X_train.shape[0]\n",
    "\n",
    "for i in range(hops,no_records):\n",
    "    X_train_list.append(X_train[i-hops:i])\n",
    "    Y_train_list.append(X_train[i])\n",
    "\n",
    "X_train_list, Y_train_list = np.array(X_train_list), np.array(Y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shaped = np.reshape(X_train_list, (X_train_list.shape[0], X_train_list.shape[1], X_train_list.shape[2]))\n",
    "Y_train_shaped = np.reshape(Y_train_list, (Y_train_list.shape[0], Y_train_list.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1047, 28, 11), (1047, 11))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_list.shape, Y_train_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1047, 28, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_shaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultidimensionalLSTM(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.dense = keras.layers.Dense(output_size)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.lstm(inputs)\n",
    "        x = self.dense(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultidimensionalLSTM(hidden_size=32, output_size=11)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "33/33 [==============================] - 2s 7ms/step - loss: 2767.0508\n",
      "Epoch 2/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 2734.8857\n",
      "Epoch 3/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 2695.7173\n",
      "Epoch 4/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 2632.5764\n",
      "Epoch 5/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 2554.1047\n",
      "Epoch 6/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 2487.3308\n",
      "Epoch 7/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 2428.3940\n",
      "Epoch 8/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 2370.9307\n",
      "Epoch 9/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 2320.9958\n",
      "Epoch 10/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 2279.5706\n",
      "Epoch 11/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 2241.2029\n",
      "Epoch 12/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 2200.6367\n",
      "Epoch 13/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 2159.1401\n",
      "Epoch 14/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 2116.0598\n",
      "Epoch 15/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 2071.4473\n",
      "Epoch 16/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 2032.1603\n",
      "Epoch 17/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1999.7969\n",
      "Epoch 18/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1970.0764\n",
      "Epoch 19/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1942.1588\n",
      "Epoch 20/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1915.0244\n",
      "Epoch 21/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1886.6991\n",
      "Epoch 22/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1858.0557\n",
      "Epoch 23/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1832.6941\n",
      "Epoch 24/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1809.5043\n",
      "Epoch 25/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1785.2842\n",
      "Epoch 26/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1762.1160\n",
      "Epoch 27/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1742.0084\n",
      "Epoch 28/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1722.6071\n",
      "Epoch 29/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1703.8141\n",
      "Epoch 30/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1686.3158\n",
      "Epoch 31/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1669.3013\n",
      "Epoch 32/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1653.0242\n",
      "Epoch 33/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1637.5365\n",
      "Epoch 34/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1622.2756\n",
      "Epoch 35/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1607.5460\n",
      "Epoch 36/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1592.4176\n",
      "Epoch 37/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1578.7045\n",
      "Epoch 38/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1564.7448\n",
      "Epoch 39/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1552.1638\n",
      "Epoch 40/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1539.6915\n",
      "Epoch 41/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1528.1112\n",
      "Epoch 42/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1516.4426\n",
      "Epoch 43/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1504.9836\n",
      "Epoch 44/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1493.7643\n",
      "Epoch 45/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1482.8707\n",
      "Epoch 46/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1472.1115\n",
      "Epoch 47/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1462.0446\n",
      "Epoch 48/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1452.0448\n",
      "Epoch 49/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1442.7476\n",
      "Epoch 50/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1433.2155\n",
      "Epoch 51/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1423.3292\n",
      "Epoch 52/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1414.2297\n",
      "Epoch 53/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1405.9669\n",
      "Epoch 54/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1396.6421\n",
      "Epoch 55/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1388.0085\n",
      "Epoch 56/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1379.7649\n",
      "Epoch 57/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1371.3118\n",
      "Epoch 58/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1364.8307\n",
      "Epoch 59/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1357.2806\n",
      "Epoch 60/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1348.8989\n",
      "Epoch 61/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1338.5547\n",
      "Epoch 62/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1330.7644\n",
      "Epoch 63/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1322.4587\n",
      "Epoch 64/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1311.6532\n",
      "Epoch 65/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1303.4443\n",
      "Epoch 66/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1294.4371\n",
      "Epoch 67/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1286.0203\n",
      "Epoch 68/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1278.9019\n",
      "Epoch 69/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1272.5978\n",
      "Epoch 70/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1265.1689\n",
      "Epoch 71/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1258.3292\n",
      "Epoch 72/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1251.5223\n",
      "Epoch 73/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1245.3298\n",
      "Epoch 74/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1238.9813\n",
      "Epoch 75/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1232.4049\n",
      "Epoch 76/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1226.6240\n",
      "Epoch 77/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1220.1997\n",
      "Epoch 78/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1213.1006\n",
      "Epoch 79/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1207.0001\n",
      "Epoch 80/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1201.0839\n",
      "Epoch 81/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1196.0511\n",
      "Epoch 82/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1190.0790\n",
      "Epoch 83/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1185.3933\n",
      "Epoch 84/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 1181.1106\n",
      "Epoch 85/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1174.6915\n",
      "Epoch 86/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1170.6837\n",
      "Epoch 87/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1165.5824\n",
      "Epoch 88/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1160.3495\n",
      "Epoch 89/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1156.2903\n",
      "Epoch 90/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1152.3097\n",
      "Epoch 91/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1147.7529\n",
      "Epoch 92/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1143.7893\n",
      "Epoch 93/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1139.0758\n",
      "Epoch 94/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1134.5840\n",
      "Epoch 95/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1132.1642\n",
      "Epoch 96/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1128.5582\n",
      "Epoch 97/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1122.0249\n",
      "Epoch 98/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1124.2814\n",
      "Epoch 99/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1118.9539\n",
      "Epoch 100/10000\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 1114.7241\n",
      "Epoch 101/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1109.4686\n",
      "Epoch 102/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1104.8479\n",
      "Epoch 103/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1102.0222\n",
      "Epoch 104/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1097.9628\n",
      "Epoch 105/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1095.0258\n",
      "Epoch 106/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1089.4292\n",
      "Epoch 107/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1085.8431\n",
      "Epoch 108/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1081.1843\n",
      "Epoch 109/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1077.1154\n",
      "Epoch 110/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1067.9587\n",
      "Epoch 111/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1066.7900\n",
      "Epoch 112/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1057.3073\n",
      "Epoch 113/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1052.1243\n",
      "Epoch 114/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 1047.4310\n",
      "Epoch 115/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1043.5129\n",
      "Epoch 116/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1039.1257\n",
      "Epoch 117/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1039.3096\n",
      "Epoch 118/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1032.7559\n",
      "Epoch 119/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1028.8804\n",
      "Epoch 120/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1025.3274\n",
      "Epoch 121/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1020.8357\n",
      "Epoch 122/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1018.9227\n",
      "Epoch 123/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1014.4815\n",
      "Epoch 124/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1010.0899\n",
      "Epoch 125/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1006.9874\n",
      "Epoch 126/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1007.9092\n",
      "Epoch 127/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 1004.0172\n",
      "Epoch 128/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 995.6420\n",
      "Epoch 129/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 993.0528\n",
      "Epoch 130/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 990.3622\n",
      "Epoch 131/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 989.1237\n",
      "Epoch 132/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 987.9363\n",
      "Epoch 133/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 982.5671\n",
      "Epoch 134/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 981.5120\n",
      "Epoch 135/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 976.7595\n",
      "Epoch 136/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 971.8432\n",
      "Epoch 137/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 971.7048\n",
      "Epoch 138/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 965.6288\n",
      "Epoch 139/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 962.8098\n",
      "Epoch 140/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 960.0554\n",
      "Epoch 141/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 958.2659\n",
      "Epoch 142/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 954.2378\n",
      "Epoch 143/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 951.7520\n",
      "Epoch 144/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 949.7750\n",
      "Epoch 145/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 949.7571\n",
      "Epoch 146/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 946.6425\n",
      "Epoch 147/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 943.8253\n",
      "Epoch 148/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 941.0458\n",
      "Epoch 149/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 938.4470\n",
      "Epoch 150/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 940.2822\n",
      "Epoch 151/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 932.5067\n",
      "Epoch 152/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 931.1722\n",
      "Epoch 153/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 926.5183\n",
      "Epoch 154/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 930.5801\n",
      "Epoch 155/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 927.9756\n",
      "Epoch 156/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 918.2783\n",
      "Epoch 157/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 915.6590\n",
      "Epoch 158/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 913.0422\n",
      "Epoch 159/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 911.5686\n",
      "Epoch 160/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 910.4924\n",
      "Epoch 161/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 906.7681\n",
      "Epoch 162/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 909.9340\n",
      "Epoch 163/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 908.6817\n",
      "Epoch 164/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 907.4605\n",
      "Epoch 165/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 905.0538\n",
      "Epoch 166/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 901.7826\n",
      "Epoch 167/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 894.2635\n",
      "Epoch 168/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 892.7892\n",
      "Epoch 169/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 891.0556\n",
      "Epoch 170/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 888.0122\n",
      "Epoch 171/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 896.4795\n",
      "Epoch 172/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 882.5845\n",
      "Epoch 173/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 883.9392\n",
      "Epoch 174/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 873.4363\n",
      "Epoch 175/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 874.8271\n",
      "Epoch 176/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 870.0005\n",
      "Epoch 177/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 863.9634\n",
      "Epoch 178/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 864.2339\n",
      "Epoch 179/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 861.2626\n",
      "Epoch 180/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 860.7593\n",
      "Epoch 181/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 856.4043\n",
      "Epoch 182/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 851.8965\n",
      "Epoch 183/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 849.1035\n",
      "Epoch 184/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 850.1661\n",
      "Epoch 185/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 848.3832\n",
      "Epoch 186/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 850.5986\n",
      "Epoch 187/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 845.2184\n",
      "Epoch 188/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 839.3994\n",
      "Epoch 189/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 838.1825\n",
      "Epoch 190/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 835.5579\n",
      "Epoch 191/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 830.8317\n",
      "Epoch 192/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 829.7021\n",
      "Epoch 193/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 824.2296\n",
      "Epoch 194/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 820.7451\n",
      "Epoch 195/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 821.2150\n",
      "Epoch 196/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 818.7034\n",
      "Epoch 197/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 818.2354\n",
      "Epoch 198/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 818.7769\n",
      "Epoch 199/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 814.9355\n",
      "Epoch 200/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 807.6861\n",
      "Epoch 201/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 809.0135\n",
      "Epoch 202/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 801.8027\n",
      "Epoch 203/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 803.2911\n",
      "Epoch 204/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 795.5212\n",
      "Epoch 205/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 797.7397\n",
      "Epoch 206/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 803.6388\n",
      "Epoch 207/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 805.2297\n",
      "Epoch 208/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 796.3923\n",
      "Epoch 209/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 797.7269\n",
      "Epoch 210/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 784.1337\n",
      "Epoch 211/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 776.6667\n",
      "Epoch 212/10000\n",
      "33/33 [==============================] - 0s 6ms/step - loss: 774.8173\n",
      "Epoch 213/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 770.7859\n",
      "Epoch 214/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 770.6345\n",
      "Epoch 215/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 766.3954\n",
      "Epoch 216/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 768.0254\n",
      "Epoch 217/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 762.9453\n",
      "Epoch 218/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 762.5406\n",
      "Epoch 219/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 774.4565\n",
      "Epoch 220/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 763.8046\n",
      "Epoch 221/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 754.3383\n",
      "Epoch 222/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 756.4510\n",
      "Epoch 223/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 752.7885\n",
      "Epoch 224/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 750.8735\n",
      "Epoch 225/10000\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 747.4004\n",
      "Epoch 226/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 743.7538\n",
      "Epoch 227/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 740.1998\n",
      "Epoch 228/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 736.8517\n",
      "Epoch 229/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 738.4124\n",
      "Epoch 230/10000\n",
      "33/33 [==============================] - 0s 7ms/step - loss: 731.0402\n",
      "Epoch 231/10000\n",
      "17/33 [==============>...............] - ETA: 0s - loss: 699.9535"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_shaped, Y_train_shaped, epochs\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Menu/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/Menu/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/Menu/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/Menu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/Menu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/Menu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/Menu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/Menu/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/Menu/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train_shaped, Y_train_shaped, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 11)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 28, 11)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hops = 28\n",
    "no_records = X_test.shape[0]\n",
    "X_test_list = []\n",
    "for i in range(hops,no_records):\n",
    "    X_test_list.append(X_test[i-hops:i])\n",
    "\n",
    "X_test_list = np.array(X_test_list)\n",
    "X_test_shaped = np.reshape(X_test_list, (X_test_list.shape[0], X_test_list.shape[1], X_test_list.shape[2]))\n",
    "X_test_shaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 12:49:31.465424: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-07-13 12:49:31.466495: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-07-13 12:49:31.467710: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "Y_test = model.predict(X_test_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06761415,  0.01626509,  0.400939  , ..., 10.505636  ,\n",
       "        -0.2613052 , -0.8870282 ],\n",
       "       [-0.05110481,  0.5250513 ,  2.3328457 , ..., -0.382154  ,\n",
       "         6.91089   , 13.112932  ],\n",
       "       [ 0.06758912, -0.04278363,  1.9145662 , ..., -1.1633902 ,\n",
       "        -1.2249805 , -1.3215911 ],\n",
       "       ...,\n",
       "       [-0.29646865,  0.19351766,  1.7236128 , ...,  8.103509  ,\n",
       "         2.6470752 ,  2.7854362 ],\n",
       "       [-0.494512  , -0.1980438 ,  1.5218685 , ...,  0.73517513,\n",
       "         0.25414658, -4.283777  ],\n",
       "       [ 0.31667835,  0.69313085,  1.9833537 , ...,  3.4063232 ,\n",
       "         3.5953965 , 13.450698  ]], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Y_test).to_csv('Y_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Menu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
